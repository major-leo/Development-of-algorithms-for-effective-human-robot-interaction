{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a400d577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import Audio, display\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "# Function to record audio\n",
    "def record_audio(duration=5, fs=44100):\n",
    "    \"\"\"\n",
    "    Record audio for a given duration and sampling rate.\n",
    "    \"\"\"\n",
    "    print(f\"Recording for {duration} seconds...\")\n",
    "    recording = sd.rec(int(duration * fs), samplerate=fs, channels=2)\n",
    "    sd.wait()\n",
    "    return recording, fs\n",
    "\n",
    "# Button to handle recording\n",
    "record_button = widgets.Button(description=\"Record Audio\")\n",
    "\n",
    "# Output widget to display audio widget\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_record_button_clicked(b):\n",
    "    with output:\n",
    "        # Clear previous recordings\n",
    "        output.clear_output()\n",
    "        \n",
    "        # Record audio\n",
    "        recording, fs = record_audio(duration=5)  # Record for 5 seconds\n",
    "        \n",
    "        # Save the recording\n",
    "        sf.write('recording.wav', recording, fs)\n",
    "        \n",
    "        # Display audio widget\n",
    "        display(Audio('recording.wav', autoplay=False))\n",
    "\n",
    "# Link button click event to the function\n",
    "record_button.on_click(on_record_button_clicked)\n",
    "\n",
    "# Display button\n",
    "display(record_button, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafacebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "# Initialize the recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Load the recorded audio file\n",
    "audio_file_path = 'temp.wav'\n",
    "\n",
    "# Perform speech recognition\n",
    "with sr.AudioFile(audio_file_path) as source:\n",
    "    # Record the audio from the file\n",
    "    audio_data = recognizer.record(source)\n",
    "\n",
    "    try:\n",
    "        # Convert speech to text\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        print(\"Recognized Text:\", text)\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Google Speech Recognition could not understand the audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Could not request results from Google Speech Recognition service; {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a33704b-0024-43e5-9725-ddec2a4103b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript: what do you think about Beethoven\n"
     ]
    }
   ],
   "source": [
    "from google.oauth2 import service_account\n",
    "from google.cloud import speech\n",
    "\n",
    "# Provide the path to your service account key file\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    'decoded-bulwark-421920-aa7f61c1c1ad.json'\n",
    ")\n",
    "\n",
    "# Pass the credentials to the speech client\n",
    "client = speech.SpeechClient(credentials=credentials)\n",
    "\n",
    "def transcribe_local_audio(file_path):\n",
    "    with open(file_path, \"rb\") as audio_file:\n",
    "        content = audio_file.read()\n",
    "    audio = speech.RecognitionAudio(content=content)\n",
    "\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code=\"en-US\"\n",
    "    )\n",
    "\n",
    "    response = client.recognize(config=config, audio=audio)\n",
    "\n",
    "    for result in response.results:\n",
    "        print(\"Transcript: {}\".format(result.alternatives[0].transcript))\n",
    "\n",
    "# Example: Call the function with your local audio file\n",
    "transcribe_local_audio('temp.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b2a4def-a802-463b-acfe-72265a54ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import texttospeech\n",
    "from google.oauth2 import service_account\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import io\n",
    "\n",
    "# Initialize the Text-to-Speech client\n",
    "client = texttospeech.TextToSpeechClient(credentials=credentials)\n",
    "\n",
    "def text_to_speech_and_play(text):\n",
    "    # Set the text input to be synthesized\n",
    "    synthesis_input = texttospeech.SynthesisInput(text=text)\n",
    "\n",
    "    # Build the voice request, select the language code and the ssml voice gender\n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code='en-US',\n",
    "        ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL\n",
    "    )\n",
    "\n",
    "    # Select the type of audio file you want\n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "    )\n",
    "\n",
    "    # Perform the text-to-speech request on the text input with the selected voice parameters and audio file type\n",
    "    response = client.synthesize_speech(\n",
    "        input=synthesis_input, voice=voice, audio_config=audio_config\n",
    "    )\n",
    "\n",
    "    # Convert the response's audio_content (binary) into an audio segment\n",
    "    audio_segment = AudioSegment.from_file(io.BytesIO(response.audio_content), format=\"mp3\")\n",
    "\n",
    "    # Play the audio segment out loud\n",
    "    play(audio_segment)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text_to_speech(\"Hello, world! This is a test of Google Cloud Text-to-Speech.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3843efd-44a8-4231-b79e-f85d113d8784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import numpy as np\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "volume = 0.5     # range [0.0, 1.0]\n",
    "fs = 44100       # sampling rate, Hz, must be integer\n",
    "duration = 1.0   # in seconds, may be float\n",
    "f = 440.0        # sine frequency, Hz, may be float\n",
    "\n",
    "# generate samples, note conversion to float32 array\n",
    "samples = (np.sin(2*np.pi*np.arange(fs*duration)*f/fs)).astype(np.float32)\n",
    "\n",
    "# for paFloat32 sample values must be in range [-1.0, 1.0]\n",
    "stream = p.open(format=pyaudio.paFloat32,\n",
    "                channels=1,\n",
    "                rate=fs,\n",
    "                output=True)\n",
    "\n",
    "# play. May repeat with different volume values (if done interactively) \n",
    "stream.write(volume*samples)\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caa31735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = 'key'\n",
    "\n",
    "def ask_chatgpt(question):\n",
    "    \"\"\"\n",
    "    Send a question to ChatGPT and return the response.\n",
    "    \"\"\"\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Assuming 'transcribed_text' contains your speech-to-text result\n",
    "gpt_response = ask_chatgpt(\"Hello ChatGPT\")\n",
    "print(gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0bfd52-5ded-4778-a4ab-e1062b85d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import webrtcvad\n",
    "import speech_recognition as sr\n",
    "from collections import deque\n",
    "\n",
    "# Constants for audio capture\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "CHUNK = int(RATE * 0.03)  # 10 ms chunk size\n",
    "RECORD_SECONDS = 7\n",
    "\n",
    "# Initialize PyAudio, VAD, and Recognizer\n",
    "audio = pyaudio.PyAudio()\n",
    "vad = webrtcvad.Vad(0)  # Level of aggressiveness from 0 to 3\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "def capture_real_time():\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                        rate=RATE, input=True,\n",
    "                        frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"Starting real-time audio capture and speech detection\")\n",
    "    frames = []\n",
    "    silent_frames = deque(maxlen=10)  # Store last few frames to add if speech starts\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            frame = stream.read(CHUNK, exception_on_overflow=False)\n",
    "            is_speech = vad.is_speech(frame, RATE)\n",
    "\n",
    "            if is_speech:\n",
    "                if silent_frames:\n",
    "                    frames.extend(silent_frames)  # Prepend silence before speech\n",
    "                    silent_frames.clear()\n",
    "                frames.append(frame)\n",
    "            else:\n",
    "                silent_frames.append(frame)\n",
    "                if len(frames) > 0:  # Speech has ended, process the captured frames\n",
    "                    with wave.open(\"test_output.wav\", \"wb\") as wf:\n",
    "                        wf.setnchannels(CHANNELS)\n",
    "                        wf.setsampwidth(audio_interface.get_sample_size(FORMAT))\n",
    "                        wf.setframerate(RATE)\n",
    "                        wf.writeframes(b''.join(frames))\n",
    "                    process_speech(frames)\n",
    "                    frames = []\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopping...\")\n",
    "    finally:\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        audio.terminate()\n",
    "\n",
    "\n",
    "\n",
    "capture_real_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524e37e0-f498-4c1d-a42d-9cc12f0f7fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import webrtcvad\n",
    "import speech_recognition as sr\n",
    "from collections import deque\n",
    "\n",
    "# Constants for audio capture\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "CHUNK = int(RATE * 0.03)  # 10 ms chunk size\n",
    "SILENT_FRAMES_BUFFER_SIZE = 30  # Store more silent frames\n",
    "\n",
    "# Initialize PyAudio, VAD, and Recognizer\n",
    "audio = pyaudio.PyAudio()\n",
    "vad = webrtcvad.Vad(0)  # Level of aggressiveness from 0 to 3\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "def capture_real_time():\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                        rate=RATE, input=True,\n",
    "                        frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"Starting real-time audio capture and speech detection\")\n",
    "    audio_frames = []\n",
    "    silent_frames = deque(maxlen=SILENT_FRAMES_BUFFER_SIZE)  # Store last few frames to add if speech starts\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Audio processing for speech detection\n",
    "            audio_frame = stream.read(CHUNK, exception_on_overflow=False)\n",
    "            is_speech = vad.is_speech(audio_frame, RATE)\n",
    "    \n",
    "            if is_speech:\n",
    "                if silent_frames:\n",
    "                    audio_frames.extend(silent_frames)\n",
    "                    silent_frames.clear()\n",
    "                audio_frames.append(audio_frame)\n",
    "            else:\n",
    "                silent_frames.append(audio_frame)\n",
    "                if len(audio_frames) > 0:\n",
    "                    detected_text = process_speech(audio_frames)\n",
    "                    audio_frames = []\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopping...\")\n",
    "    finally:\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        audio.terminate()\n",
    "\n",
    "def process_speech(frames):\n",
    "    # Write frames to a buffer\n",
    "    buffer = wave.open('temp.wav', 'wb')\n",
    "    buffer.setnchannels(CHANNELS)\n",
    "    buffer.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "    buffer.setframerate(RATE)\n",
    "    buffer.writeframes(b''.join(frames))\n",
    "    buffer.close()\n",
    "\n",
    "    # Use speech recognition on the buffer\n",
    "    with sr.AudioFile('temp.wav') as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            print(f\"Recognized text: {text}\")\n",
    "        except sr.UnknownValueError:\n",
    "            pass\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Error from speech recognition service: {e}\")\n",
    "\n",
    "capture_real_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e79adc-4d83-450f-9c63-8399a81c0d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3db2e10-935c-47e9-af9c-54a5af231c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "Recognized text: what do you think of beeth\n",
      "Stopping...\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import webrtcvad\n",
    "import speech_recognition as sr\n",
    "\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "CHUNK = 320  # This corresponds to 20 ms of audio at 16000 Hz\n",
    "\n",
    "vad = webrtcvad.Vad(1)  # Moderate aggressiveness\n",
    "audio_interface = pyaudio.PyAudio()\n",
    "\n",
    "def listen_and_process():\n",
    "    stream = audio_interface.open(format=FORMAT, channels=CHANNELS,\n",
    "                                  rate=RATE, input=True,\n",
    "                                  frames_per_buffer=CHUNK)\n",
    "    print(\"Listening...\")\n",
    "\n",
    "    speech_frames = bytes()  # buffer to collect frames identified as speech\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            frame = stream.read(CHUNK, exception_on_overflow=False)  # Read a frame from the stream\n",
    "            is_speech = vad.is_speech(frame, RATE)  # Check if the frame contains speech\n",
    "\n",
    "            if is_speech:\n",
    "                speech_frames += frame  # Accumulate speech frames\n",
    "\n",
    "            elif speech_frames:\n",
    "                # When non-speech detected and there are accumulated speech frames\n",
    "                process_speech(speech_frames)\n",
    "                speech_frames = bytes()  # Reset speech frame buffer\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopping...\")\n",
    "    finally:\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        audio_interface.terminate()\n",
    "\n",
    "def process_speech(frames):\n",
    "    # Process the speech frames\n",
    "    recognizer = sr.Recognizer()\n",
    "    \n",
    "    # Create an instance of AudioData\n",
    "    audio_data = sr.AudioData(frames, RATE, audio_interface.get_sample_size(FORMAT))\n",
    "    \n",
    "    try:\n",
    "        # Recognize speech using the AudioData instance\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        print(f\"Recognized text: {text}\")\n",
    "    except sr.UnknownValueError:\n",
    "        # print(\"Speech was not understood.\")\n",
    "        pass\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Error from speech recognition service: {e}\")\n",
    "\n",
    "listen_and_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0db8c4-0f2d-4599-869b-f5106d41f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text):\n",
    "    # Set the text input to be synthesized\n",
    "    synthesis_input = texttospeech.SynthesisInput(text=text)\n",
    "\n",
    "    # Build the voice request, select the language code and the voice type\n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code='en-US',\n",
    "        ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL\n",
    "    )\n",
    "\n",
    "    # Select the type of audio file you want\n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "    )\n",
    "\n",
    "    # Perform the text-to-speech request on the text input with the selected voice parameters and audio file type\n",
    "    response = client.synthesize_speech(\n",
    "        input=synthesis_input, voice=voice, audio_config=audio_config\n",
    "    )\n",
    "\n",
    "    # The response's audio_content is binary.\n",
    "    with open('output.mp3', 'wb') as out:\n",
    "        # Write the response to the output file.\n",
    "        out.write(response.audio_content)\n",
    "        print('Audio content written to file \"output.mp3\"')\n",
    "\n",
    "# Example usage\n",
    "text_to_speech(\"Hello, world! This is a test of Google Cloud Text-to-Speech.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2bc336-f641-4812-bbf0-eeae25b35988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
