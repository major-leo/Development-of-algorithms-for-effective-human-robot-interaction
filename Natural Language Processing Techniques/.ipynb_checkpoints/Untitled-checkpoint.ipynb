{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a55d9a6c-c69f-4d08-8efc-92051d7d912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_head_pose(landmarks, image_shape):\n",
    "    # creates a 3d coordinate system model of a face\n",
    "    model_points = np.array([\n",
    "        (0.0, 0.0, 0.0),             # nose tip\n",
    "        (0.0, -330.0, -65.0),        # chin\n",
    "        (-225.0, 170.0, -135.0),     # left eye left corner\n",
    "        (225.0, 170.0, -135.0),      # right eye right corner\n",
    "        (-150.0, -150.0, -125.0),    # left Mouth corner\n",
    "        (150.0, -150.0, -125.0)      # right mouth corner\n",
    "    ])\n",
    "    \n",
    "    # using media pipe face gets the point corresponding to the model face\n",
    "    image_points = np.array([\n",
    "        (landmarks[1].x * image_shape[1], landmarks[1].y * image_shape[0]),  # nose tip\n",
    "        (landmarks[152].x * image_shape[1], landmarks[152].y * image_shape[0]),  # chin\n",
    "        (landmarks[226].x * image_shape[1], landmarks[226].y * image_shape[0]),  # left eye left corner\n",
    "        (landmarks[446].x * image_shape[1], landmarks[446].y * image_shape[0]),  # right eye right corner\n",
    "        (landmarks[57].x * image_shape[1], landmarks[57].y * image_shape[0]),  # left Mouth corner\n",
    "        (landmarks[287].x * image_shape[1], landmarks[287].y * image_shape[0])  # right mouth corner\n",
    "    ], dtype=\"double\")\n",
    "\n",
    "    # calculates the camera matrix\n",
    "    principal_point = (image_shape[1]/2, image_shape[0]/2)\n",
    "    focal_length = principal_point[0] / np.tan(60 / 2 * np.pi / 180)\n",
    "    camera_matrix = np.array(\n",
    "        [[focal_length, 0, principal_point[0]],\n",
    "         [0, focal_length, principal_point[1]],\n",
    "         [0, 0, 1]], dtype = \"double\"\n",
    "    )\n",
    "\n",
    "    # assuming no lens distortion\n",
    "    distCoeffs = np.zeros((4,1))\n",
    "\n",
    "    # SolvePnP\n",
    "    (success, rotation_vector, translation_vector) = cv2.solvePnP(model_points, image_points, camera_matrix, distCoeffs, flags=cv2.SOLVEPNP_ITERATIVE)\n",
    "\n",
    "    return rotation_vector, translation_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26a249b9-f535-4bdb-8448-5ef77b1e934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_vector_to_euler_angles(rotation_vector):\n",
    "    # converts a rotation vector to a rotation matrix\n",
    "    rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
    "    # append a zero column to make it a 3x4 matrix\n",
    "    projection_matrix = np.hstack((rotation_matrix, np.zeros((3, 1))))\n",
    "    # returns three-element vector containing three Euler angles of rotation in degrees.\n",
    "    euler_angles = cv2.decomposeProjectionMatrix(projection_matrix)[6]\n",
    "    pitch, yaw, roll = [angle for angle in euler_angles]\n",
    "    \n",
    "    return pitch, yaw, roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1d8d117-23de-4ee5-b90b-b0fb614cd532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_facing_camera(pitch, yaw, pitch_threshold=160, yaw_threshold=25):\n",
    "    if abs(pitch) > pitch_threshold and abs(yaw) < yaw_threshold:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fb77a44-0d1a-4545-ae3e-2922e1fc60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_history = []\n",
    "engagement_history = []\n",
    "\n",
    "def update_history(emotion, engaged, history_length=10):\n",
    "    emotion_history.append(emotion)\n",
    "    engagement_history.append(engaged)\n",
    "    if len(emotion_history) > history_length:\n",
    "        emotion_history.pop(0)\n",
    "        engagement_history.pop(0)\n",
    "\n",
    "def analyze_interest():\n",
    "    if all(not e for e in engagement_history[-3:]) and 'sad' in emotion_history[-3:]:\n",
    "        return \"It seems like you're not very interested. Would you like to talk about something else?\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f54eaa2-49de-4ea8-9476-ea0b975105dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_chatgpt(question):\n",
    "    \"\"\"\n",
    "    Send a question to ChatGPT and return the response.\n",
    "    \"\"\"\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb2960a7-b561-4364-be71-a48e2c8193ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('model_all.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d93d5e2b-85af-413a-9a38-9883f5f15846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_speech(frames):\n",
    "    # Process the speech frames\n",
    "    recognizer = sr.Recognizer()\n",
    "    \n",
    "    # Create an instance of AudioData\n",
    "    audio_data = sr.AudioData(frames, RATE, audio_interface.get_sample_size(FORMAT))\n",
    "    \n",
    "    try:\n",
    "        # Recognize speech using the AudioData instance\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        print(f\"Recognized text: {text}\")\n",
    "    except sr.UnknownValueError:\n",
    "        # print(\"Speech was not understood.\")\n",
    "        pass\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Error from speech recognition service: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d362074a-41cc-40fb-8dd6-9d393af72a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Stream stopped\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import openai\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import speech_recognition as sr\n",
    "import openai\n",
    "import pyaudio\n",
    "import webrtcvad\n",
    "import speech_recognition as sr\n",
    "\n",
    "# FORMAT = pyaudio.paInt16\n",
    "# CHANNELS = 1\n",
    "# RATE = 16000\n",
    "# CHUNK = 320  # This corresponds to 20 ms of audio at 16000 Hz\n",
    "\n",
    "# vad = webrtcvad.Vad(1)  # Moderate aggressiveness\n",
    "# audio_interface = pyaudio.PyAudio()\n",
    "\n",
    "openai.api_key = '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'\n",
    "\n",
    "# Initialize MediaPipe Face Detection.\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)\n",
    "emotions_dict = {\n",
    "    0: \"angry\",\n",
    "    1: \"disgust\",\n",
    "    2: \"fear\",\n",
    "    3: \"happy\",\n",
    "    4: \"neutral\",\n",
    "    5: \"sad\",\n",
    "    6: \"surprised\"\n",
    "}\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Initialize Video Capture and Video Writer.\n",
    "cap = cv2.VideoCapture(1)\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "# out = cv2.VideoWriter('output.avi', fourcc, 20.0, (640, 480))\n",
    "\n",
    "# stream = audio_interface.open(format=FORMAT, channels=CHANNELS,\n",
    "#                               rate=RATE, input=True,\n",
    "#                               frames_per_buffer=CHUNK)\n",
    "# print(\"Listening...\")\n",
    "\n",
    "speech_frames = bytes()  # buffer to collect frames identified as speech\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the frame to RGB before processing.\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        detection_results = face_detection.process(frame_rgb)\n",
    "        mesh_results = face_mesh.process(frame_rgb)\n",
    "\n",
    "        if detection_results.detections:\n",
    "            for detection in detection_results.detections:\n",
    "                # Get the bounding box.\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                ih, iw, ic = frame.shape\n",
    "                x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "\n",
    "                x, y = max(0, x), max(0, y)\n",
    "                w, h = min(iw - x, w), min(ih - y, h)\n",
    "\n",
    "                if w > 0 and h > 0:\n",
    "                    face_roi = frame[y:y+h, x:x+w]\n",
    "                    if face_roi.size == 0:\n",
    "                        continue  # Skip if the ROI is empty\n",
    "\n",
    "                # Extract the face ROI.\n",
    "                face_roi = frame[y:y+h, x:x+w]\n",
    "            \n",
    "                # In case you want to save or display the face ROI:\n",
    "                # cv2.imwrite('face.jpg', face_roi)\n",
    "                # cv2.imshow('Face ROI', face_roi)\n",
    "                \n",
    "                gray_frame = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                # Draw the rectangle around the face on the frame.\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "                # Optionally: Use `mp_drawing.draw_detection` to draw the detection.\n",
    "                # This will draw the full detection box, landmarks, and classification score.\n",
    "                # mp_drawing.draw_detection(frame, detection)\n",
    "\n",
    "                 # Resize the frame to match the input size of the model (e.g., 48x48 for FER-2013)\n",
    "                resized_frame = cv2.resize(gray_frame, (48, 48))\n",
    "                \n",
    "                # Normalize the frame\n",
    "                normalized_frame = resized_frame.astype('float32') / 255.0\n",
    "                \n",
    "                # Expand dimensions to match the model's input format (batch_size, height, width, channels)\n",
    "                # Assuming the model expects a 4D input\n",
    "                expanded_frame = np.expand_dims(normalized_frame, axis=0)\n",
    "                expanded_frame = np.expand_dims(expanded_frame, axis=-1)\n",
    "                \n",
    "                # Predict the emotion on the expanded frame\n",
    "                prediction = model.predict(expanded_frame)\n",
    "                # Assuming your model returns a list of predictions\n",
    "                emotion_label = np.argmax(prediction)\n",
    "                \n",
    "                # Display the resulting frame with detected emotion\n",
    "                cv2.putText(frame, f'Emotion: {emotions_dict[emotion_label]}', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                \n",
    "         # draw the face mesh mapping onto the frame\n",
    "        if mesh_results.multi_face_landmarks:\n",
    "            for face_landmarks in mesh_results.multi_face_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image=frame,\n",
    "                    landmark_list=face_landmarks,\n",
    "                    connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                    landmark_drawing_spec=drawing_spec,\n",
    "                    connection_drawing_spec=drawing_spec)\n",
    "                \n",
    "                 # get rotation vector\n",
    "                rotation_vector, translation_vector = get_head_pose(face_landmarks.landmark, frame.shape)\n",
    "                # get pitch and yaw\n",
    "                pitch, yaw, _ = rotation_vector_to_euler_angles(rotation_vector)\n",
    "                # check if user is facing the camera\n",
    "                if is_facing_camera(pitch, yaw):\n",
    "                    cv2.putText(frame, \"true\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                else:\n",
    "                    cv2.putText(frame, \"false\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        # audio_frame = stream.read(CHUNK, exception_on_overflow=False)  # Read a frame from the stream\n",
    "        # is_speech = vad.is_speech(audio_frame, RATE)  # Check if the frame contains speech\n",
    "\n",
    "        # if is_speech:\n",
    "        #     speech_frames += audio_frame  # Accumulate speech frames\n",
    "\n",
    "        # elif speech_frames:\n",
    "        #     # When non-speech detected and there are accumulated speech frames\n",
    "        #     process_speech(speech_frames)\n",
    "        #     speech_frames = bytes()  # Reset speech frame buffer\n",
    "            \n",
    "        cv2.imshow('Frame', frame)\n",
    "\n",
    "        # # Write the frame to the output file.\n",
    "        # out.write(frame)\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stream stopped\")\n",
    "\n",
    "finally:\n",
    "    # Release resources.\n",
    "    cap.release()\n",
    "    # out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    # stream.stop_stream()\n",
    "    # stream.close()\n",
    "    # audio_interface.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9855ece3-c415-4eee-a393-1b3c7f2f51ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2e3562-0b1f-4bfc-b09a-b1b0cfc43e53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
