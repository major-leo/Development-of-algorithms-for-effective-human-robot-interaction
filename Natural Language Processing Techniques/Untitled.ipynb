{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a55d9a6c-c69f-4d08-8efc-92051d7d912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_head_pose(landmarks, image_shape):\n",
    "    # creates a 3d coordinate system model of a face\n",
    "    model_points = np.array([\n",
    "        (0.0, 0.0, 0.0),             # nose tip\n",
    "        (0.0, -330.0, -65.0),        # chin\n",
    "        (-225.0, 170.0, -135.0),     # left eye left corner\n",
    "        (225.0, 170.0, -135.0),      # right eye right corner\n",
    "        (-150.0, -150.0, -125.0),    # left Mouth corner\n",
    "        (150.0, -150.0, -125.0)      # right mouth corner\n",
    "    ])\n",
    "    \n",
    "    # using media pipe face gets the point corresponding to the model face\n",
    "    image_points = np.array([\n",
    "        (landmarks[1].x * image_shape[1], landmarks[1].y * image_shape[0]),  # nose tip\n",
    "        (landmarks[152].x * image_shape[1], landmarks[152].y * image_shape[0]),  # chin\n",
    "        (landmarks[226].x * image_shape[1], landmarks[226].y * image_shape[0]),  # left eye left corner\n",
    "        (landmarks[446].x * image_shape[1], landmarks[446].y * image_shape[0]),  # right eye right corner\n",
    "        (landmarks[57].x * image_shape[1], landmarks[57].y * image_shape[0]),  # left Mouth corner\n",
    "        (landmarks[287].x * image_shape[1], landmarks[287].y * image_shape[0])  # right mouth corner\n",
    "    ], dtype=\"double\")\n",
    "\n",
    "    # calculates the camera matrix\n",
    "    principal_point = (image_shape[1]/2, image_shape[0]/2)\n",
    "    focal_length = principal_point[0] / np.tan(60 / 2 * np.pi / 180)\n",
    "    camera_matrix = np.array(\n",
    "        [[focal_length, 0, principal_point[0]],\n",
    "         [0, focal_length, principal_point[1]],\n",
    "         [0, 0, 1]], dtype = \"double\"\n",
    "    )\n",
    "\n",
    "    # assuming no lens distortion\n",
    "    distCoeffs = np.zeros((4,1))\n",
    "\n",
    "    # SolvePnP\n",
    "    (success, rotation_vector, translation_vector) = cv2.solvePnP(model_points, image_points, camera_matrix, distCoeffs, flags=cv2.SOLVEPNP_ITERATIVE)\n",
    "\n",
    "    return rotation_vector, translation_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26a249b9-f535-4bdb-8448-5ef77b1e934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_vector_to_euler_angles(rotation_vector):\n",
    "    # converts a rotation vector to a rotation matrix\n",
    "    rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
    "    # append a zero column to make it a 3x4 matrix\n",
    "    projection_matrix = np.hstack((rotation_matrix, np.zeros((3, 1))))\n",
    "    # returns three-element vector containing three Euler angles of rotation in degrees.\n",
    "    euler_angles = cv2.decomposeProjectionMatrix(projection_matrix)[6]\n",
    "    pitch, yaw, roll = [angle for angle in euler_angles]\n",
    "    \n",
    "    return pitch, yaw, roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1d8d117-23de-4ee5-b90b-b0fb614cd532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_facing_camera(pitch, yaw, pitch_threshold=160, yaw_threshold=25):\n",
    "    if abs(pitch) > pitch_threshold and abs(yaw) < yaw_threshold:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fb77a44-0d1a-4545-ae3e-2922e1fc60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_history(emotion, engaged, history_length=5):\n",
    "    emotion_history.append(emotion)\n",
    "    engagement_history.append(engaged)\n",
    "    if len(emotion_history) > history_length:\n",
    "        emotion_history.pop(0)\n",
    "        engagement_history.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f54eaa2-49de-4ea8-9476-ea0b975105dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_chatgpt(question):\n",
    "    \"\"\"\n",
    "    Send a question to ChatGPT and return the response.\n",
    "    \"\"\"\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are talking to the user with text to speech and vice versa\\\n",
    "            and you will be given the users facial expression where\\\n",
    "            {0: angry, 1: disgust, 2: fear, 3: happy, 4: neutral, 5: sad, 6: surprised}\\\n",
    "            and if the user is looking at the camera.\\\n",
    "            You will also be given the conversation log to keep track of the conversation.\\\n",
    "            Information in bracket is information given by the system.\\\n",
    "            Focus on the information outside of the brackets first which is information from the user.\\\n",
    "            Use this information to act like a friend.\\\n",
    "            Ocasionally take initiative and ask a question about the user.\\\n",
    "            Only talk about the user's feeling if their feeling is not neutral\"}, \n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b10fd14-a085-4990-8be3-9d20d14a4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_speech(data):\n",
    "    audio = speech.RecognitionAudio(content=data)\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=RATE,\n",
    "        language_code=\"en-US\"\n",
    "    )\n",
    "    response = speech_client.recognize(config=config, audio=audio)\n",
    "    if response.results:\n",
    "        return response.results[0].alternatives[0].transcript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeb6c9ce-2111-4032-996c-bb7cb67fbffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chatgpt_prompt(text, emotion, is_engaged, conv_log):\n",
    "    \"\"\"\n",
    "    Generate a prompt for ChatGPT based on user's current emotional state and engagement.\n",
    "    \"\"\"\n",
    "        \n",
    "    prompt = text\n",
    "    prompt += \" (User engagement is shown by the following list in chronological order \"+ str(is_engaged)\\\n",
    "    + \", and the user's feeling are shown by the following list in chronological order \" + str(emotion) +\\\n",
    "    \"the conversation log is: \" + '\\n'.join(conv_log) + \")\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d644ede7-6c2b-4956-871c-d2f5759e94bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_history(emotion, engaged, history_length=3):\n",
    "    emotion_history.append(emotion)\n",
    "    engagement_history.append(engaged)\n",
    "    if len(emotion_history) > history_length:\n",
    "        emotion_history.pop(0)\n",
    "        engagement_history.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1a9b22f-a95c-4fa8-b495-ecb0ed3abf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech_and_play(text):\n",
    "    # Set the text input to be synthesized\n",
    "    synthesis_input = texttospeech.SynthesisInput(text=text)\n",
    "\n",
    "    # Build the voice request, select the language code and the ssml voice gender\n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code='en-US',\n",
    "        ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL\n",
    "    )\n",
    "\n",
    "    # Select the type of audio file you want\n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "    )\n",
    "\n",
    "    # Perform the text-to-speech request on the text input with the selected voice parameters and audio file type\n",
    "    response = text_to_speech_client.synthesize_speech(\n",
    "        input=synthesis_input, voice=voice, audio_config=audio_config\n",
    "    )\n",
    "\n",
    "    # Convert the response's audio_content (binary) into an audio segment\n",
    "    audio_segment = AudioSegment.from_file(io.BytesIO(response.audio_content), format=\"mp3\")\n",
    "\n",
    "    # Play the audio segment out loud\n",
    "    play(audio_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb2960a7-b561-4364-be71-a48e2c8193ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('model_all.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf2e3562-0b1f-4bfc-b09a-b1b0cfc43e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized text: the frames like really bad cuz like um\n",
      "Sounds like you had a tough time with something, but I noticed you're still trying to stay positive. What happened that made the frames turn out so bad?\n",
      "Recognized text: brings bad because if I do like Max frame rate or like high frame rate\n",
      "Oh, I see. So adjusting the Max frame rate or high frame rate didn't really work out as you expected. What were you trying to doâ€”playing a game or working on some sort of project?\n",
      "Stream stopped\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import speech\n",
    "import pyaudio\n",
    "import webrtcvad\n",
    "from collections import deque\n",
    "import wave\n",
    "import io\n",
    "from google.cloud import texttospeech\n",
    "from google.oauth2 import service_account\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import openai\n",
    "\n",
    "# Provide the path to your service account key file\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    'decoded-bulwark-421920-aa7f61c1c1ad.json'\n",
    ")\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = 'key'\n",
    "\n",
    "# Initialize the Text-to-Speech client\n",
    "text_to_speech_client  = texttospeech.TextToSpeechClient(credentials=credentials)\n",
    "\n",
    "emotion_history = []\n",
    "engagement_history = []\n",
    "conv_log = []\n",
    "\n",
    "# Setup for audio and speech recognition\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "CHUNK = int(RATE * 0.03)  # 10 ms chunk size\n",
    "audio_interface = pyaudio.PyAudio()\n",
    "stream = audio_interface.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK)\n",
    "vad = webrtcvad.Vad(1)\n",
    "# Pass the credentials to the speech client\n",
    "speech_client  = speech.SpeechClient(credentials=credentials)\n",
    "# Increase the size of silent frames buffer\n",
    "SILENT_FRAMES_BUFFER_SIZE = 30  # Store more silent frames\n",
    "\n",
    "# MediaPipe setup for face detection and mesh\n",
    "frame_skip = 5  # Skip every 2 frames\n",
    "frame_count = 0\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)\n",
    "face_mesh = mp.solutions.face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "\n",
    "# Emotion dictionary and video capture setup\n",
    "\n",
    "emotions_dict = {0: \"angry\", 1: \"disgust\", 2: \"fear\", 3: \"happy\", 4: \"neutral\", 5: \"sad\", 6: \"surprised\"}\n",
    "cap = cv2.VideoCapture(1)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "try:\n",
    "    silent_frames = deque(maxlen=SILENT_FRAMES_BUFFER_SIZE)   # Buffer to store silent frames\n",
    "    audio_frames = []\n",
    "    detected_text = \"\"\n",
    "    emotion_label = 4\n",
    "    engagement = True\n",
    "    \n",
    "\n",
    "    while True:\n",
    "\n",
    "        # Audio processing for speech detection\n",
    "        audio_frame = stream.read(CHUNK, exception_on_overflow=False)\n",
    "        is_speech = vad.is_speech(audio_frame, RATE)\n",
    "\n",
    "        if is_speech:\n",
    "            if silent_frames:\n",
    "                audio_frames.extend(silent_frames)\n",
    "                silent_frames.clear()\n",
    "            audio_frames.append(audio_frame)\n",
    "        else:\n",
    "            silent_frames.append(audio_frame)\n",
    "            if len(audio_frames) > 0:\n",
    "                detected_text = process_speech(b''.join(audio_frames))\n",
    "                if detected_text != \"\" and detected_text:\n",
    "                    print(f\"Recognized text: {detected_text}\")\n",
    "                    prompt = generate_chatgpt_prompt(detected_text, emotion_history, engagement_history, conv_log)\n",
    "                    response = ask_chatgpt(prompt)\n",
    "                    print(response)\n",
    "                    text_to_speech_and_play(response)\n",
    "                    if len(conv_log) > 4:\n",
    "                        conv_log.pop(0)\n",
    "                        conv_log.pop(0)\n",
    "                    conv_log.append(f\"Recognized text: {detected_text}\")\n",
    "                    conv_log.append(f\"Response text: {response}\")\n",
    "                audio_frames = []\n",
    "\n",
    "        \n",
    "        \n",
    "        ret, frame = cap.read()            \n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        frame_count += 1\n",
    "        \n",
    "        if frame_count % frame_skip != 0:\n",
    "            continue  # Skip this frame\n",
    "        # Video processing for face detection\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        detection_results = face_detection.process(frame_rgb)\n",
    "        mesh_results = face_mesh.process(frame_rgb)\n",
    "        frame = cv2.resize(frame, (640, 480))\n",
    "\n",
    "        # Handling face detection\n",
    "        if detection_results.detections:\n",
    "            for detection in detection_results.detections:\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                ih, iw, ic = frame.shape\n",
    "                x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "                face_roi = frame[y:y+h, x:x+w] if x >= 0 and y >= 0 else None\n",
    "                if face_roi is not None and face_roi.size > 0:\n",
    "                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "                    # Emotion detection\n",
    "                    gray_frame = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "                    resized_frame = cv2.resize(gray_frame, (48, 48))\n",
    "                    normalized_frame = resized_frame.astype('float32') / 255.0\n",
    "                    expanded_frame = np.expand_dims(np.expand_dims(normalized_frame, axis=0), axis=-1)\n",
    "                    prediction = model.predict(expanded_frame, verbose=0)\n",
    "                    emotion_label = np.argmax(prediction)\n",
    "\n",
    "        # Handling face mesh\n",
    "        if mesh_results.multi_face_landmarks:\n",
    "            for face_landmarks in mesh_results.multi_face_landmarks:\n",
    "                # mp_drawing.draw_landmarks(\n",
    "                #     image=frame,\n",
    "                #     landmark_list=face_landmarks,\n",
    "                #     connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
    "                #     landmark_drawing_spec=drawing_spec,\n",
    "                #     connection_drawing_spec=drawing_spec)\n",
    "                # get rotation vector\n",
    "                rotation_vector, translation_vector = get_head_pose(face_landmarks.landmark, frame.shape)\n",
    "                # get pitch and yaw\n",
    "                pitch, yaw, _ = rotation_vector_to_euler_angles(rotation_vector)\n",
    "                # check if user is facing the camera\n",
    "                if is_facing_camera(pitch, yaw):\n",
    "                    engagement = True\n",
    "                    \n",
    "                else:\n",
    "                    engagement = False\n",
    "\n",
    "        update_history(emotion_label, engagement)\n",
    "        cv2.putText(frame, str(engagement), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f'Emotion: {emotions_dict[emotion_label]}', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('Frame', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stream stopped\")\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio_interface.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef2ee8f-cd3a-42f0-a715-361f6ba6a4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
