{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6571856b-7a86-4b1a-9ecc-f9fa0023be20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('model_all.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c1e12-e640-4ab9-bf50-365ba4dbaca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe Face Detection.\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)\n",
    "emotions_dict = {\n",
    "    0: \"angry\",\n",
    "    1: \"disgust\",\n",
    "    2: \"fear\",\n",
    "    3: \"happy\",\n",
    "    4: \"neutral\",\n",
    "    5: \"sad\",\n",
    "    6: \"surprised\"\n",
    "}\n",
    "\n",
    "# Initialize Video Capture and Video Writer.\n",
    "cap = cv2.VideoCapture(1)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.avi', fourcc, 20.0, (640, 480))\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the frame to RGB before processing.\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_detection.process(frame_rgb)\n",
    "\n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                # Get the bounding box.\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                ih, iw, ic = frame.shape\n",
    "                x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "\n",
    "                # Extract the face ROI.\n",
    "                face_roi = frame[y:y+h, x:x+w]\n",
    "            \n",
    "                # In case you want to save or display the face ROI:\n",
    "                # cv2.imwrite('face.jpg', face_roi)\n",
    "                # cv2.imshow('Face ROI', face_roi)\n",
    "                \n",
    "                gray_frame = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                # Draw the rectangle around the face on the frame.\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "                # Optionally: Use `mp_drawing.draw_detection` to draw the detection.\n",
    "                # This will draw the full detection box, landmarks, and classification score.\n",
    "                # mp_drawing.draw_detection(frame, detection)\n",
    "\n",
    "                 # Resize the frame to match the input size of the model (e.g., 48x48 for FER-2013)\n",
    "                resized_frame = cv2.resize(gray_frame, (48, 48))\n",
    "                \n",
    "                # Normalize the frame\n",
    "                normalized_frame = resized_frame.astype('float32') / 255.0\n",
    "                \n",
    "                # Expand dimensions to match the model's input format (batch_size, height, width, channels)\n",
    "                # Assuming the model expects a 4D input\n",
    "                expanded_frame = np.expand_dims(normalized_frame, axis=0)\n",
    "                expanded_frame = np.expand_dims(expanded_frame, axis=-1)\n",
    "                \n",
    "                # Predict the emotion on the expanded frame\n",
    "                prediction = model.predict(expanded_frame)\n",
    "                # Assuming your model returns a list of predictions\n",
    "                emotion_label = np.argmax(prediction)\n",
    "                \n",
    "                # Display the resulting frame with detected emotion\n",
    "                cv2.putText(frame, f'Emotion: {emotions_dict[emotion_label]}', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Show the frame with the face detection.\n",
    "        cv2.imshow('Frame', frame)\n",
    "\n",
    "        # Write the frame to the output file.\n",
    "        out.write(frame)\n",
    "\n",
    "        # Press 'q' to exit the loop.\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stream stopped\")\n",
    "\n",
    "finally:\n",
    "    # Release resources.\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
